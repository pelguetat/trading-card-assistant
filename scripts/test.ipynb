{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLOWorld\n",
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Load the YOLOv8 model\n",
    "    model = YOLOWorld(\n",
    "        \"/Users/pabloelgueta/Documents/trading-card-assistant/yolov8l-worldv2.pt\"\n",
    "    )\n",
    "    custom_classes = [\"trading_card\"]\n",
    "    model.set_classes(custom_classes)\n",
    "    model.save(\"custom_yolo_world.pt\")\n",
    "    model = YOLOWorld(\"custom_yolo_world.pt\")\n",
    "    # Export the model to CoreML format\n",
    "    model.export(format=\"coreml\", nms=True)  # creates 'yolov8n.mlpackage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/pabloelgueta/Documents/trading-card-assistant/scripts/doughnuts-donuts.jpeg: 448x640 15 donutss, 446.0ms\n",
      "Speed: 4.8ms preprocess, 446.0ms inference, 3.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1m/Users/pabloelgueta/Documents/trading_card_detection/runs/detect/predict2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLOWorld\n",
    "\n",
    "model = YOLOWorld(\"custom_yolo_world.pt\")\n",
    "model.set_classes([\"donuts\"])\n",
    "results = model.predict(\n",
    "    \"/Users/pabloelgueta/Documents/trading-card-assistant/scripts/doughnuts-donuts.jpeg\",\n",
    "    save=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "model = ct.models.MLModel(\"custom_yolo_world.mlpackage\")\n",
    "spec = model.get_spec()\n",
    "# spec.description.metadata.userDefined[\"names\"]=list_to_dict_string(classes)\n",
    "# for k in range(len(classes)):\n",
    "#    spec.pipeline.models[1].nonMaximumSuppression.stringClassLabels.vector[k]=classes[k]\n",
    "spec.pipeline.models[1].nonMaximumSuppression.iouThreshold = 0.45  # default 0.45\n",
    "spec.pipeline.models[1].nonMaximumSuppression.confidenceThreshold = 0.25  # default 0.25\n",
    "\n",
    "model = ct.models.MLModel(spec, weights_dir=model.weights_dir)\n",
    "model.save(\"MyModel.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "{\n    NSLocalizedDescription = \"Failed to evaluate model 0 in pipeline\";\n    NSUnderlyingError = \"Error Domain=com.apple.CoreML Code=0 \\\"Image size 1600 x 1067 not in allowed set of image sizes\\\" UserInfo={NSLocalizedDescription=Image size 1600 x 1067 not in allowed set of image sizes}\";\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mMLModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_yolo_world.mlpackage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mexample_image\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/trading-card-assistant/.venv/lib/python3.11/site-packages/coremltools/models/model.py:627\u001b[0m, in \u001b[0;36mMLModel.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    624\u001b[0m MLModel\u001b[38;5;241m.\u001b[39m_check_predict_data(data)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__proxy__:\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMLModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__proxy__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_and_convert_input_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m# Error case\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _macos_version() \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m13\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/trading-card-assistant/.venv/lib/python3.11/site-packages/coremltools/models/model.py:670\u001b[0m, in \u001b[0;36mMLModel._get_predictions\u001b[0;34m(proxy, preprocess_method, data)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    669\u001b[0m     preprocess_method(data)\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mproxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: {\n    NSLocalizedDescription = \"Failed to evaluate model 0 in pipeline\";\n    NSUnderlyingError = \"Error Domain=com.apple.CoreML Code=0 \\\"Image size 1600 x 1067 not in allowed set of image sizes\\\" UserInfo={NSLocalizedDescription=Image size 1600 x 1067 not in allowed set of image sizes}\";\n}"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "example_image = Image.open(\"/Users/pabloelgueta/Documents/trading-card-assistant/scripts/doughnuts-donuts.jpeg\")\n",
    "model = ct.models.MLModel(\"custom_yolo_world.mlpackage\")\n",
    "\n",
    "\n",
    "# Run inference\n",
    "results = model.predict(\n",
    "    {\"image\":example_image}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_images_from_json(json_file_path):\n",
    "    # Directory to save the images\n",
    "    images_dir = 'images'\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "    # Function to download an image\n",
    "    def download_image(url, save_path):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, 'wb') as img_file:\n",
    "                img_file.write(response.content)\n",
    "            print(f\"Downloaded {save_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {url}\")\n",
    "\n",
    "    # Load the JSON data\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        cards = json.load(json_file)\n",
    "\n",
    "    # Iterate over each card in the JSON list\n",
    "    for card in cards:\n",
    "        # Download the large image for the card\n",
    "        card_id = card['id']\n",
    "        image_url = card['images']['large']\n",
    "        save_path = os.path.join(images_dir, f\"{card_id}.png\")\n",
    "        download_image(image_url, save_path)\n",
    "\n",
    "# Example usage\n",
    "download_images_from_json('pokemon-tcg-data-master/cards/en/sm115.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_metadata(metadata):\n",
    "    flat_metadata = {}\n",
    "\n",
    "    def _flatten(obj, parent_key=\"\"):\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                new_key = f\"{parent_key}_{k}\" if parent_key else k\n",
    "                _flatten(v, new_key)\n",
    "        elif isinstance(obj, list):\n",
    "            for i, v in enumerate(obj):\n",
    "                new_key = f\"{parent_key}_{i}\" if parent_key else str(i)\n",
    "                _flatten(v, new_key)\n",
    "        else:\n",
    "            flat_metadata[parent_key] = obj\n",
    "\n",
    "    _flatten(metadata)\n",
    "    return flat_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import chromadb\n",
    "\n",
    "\n",
    "def vectorize_cards(image_dir, json_file_path):\n",
    "    # Load the ChromaDB client\n",
    "    chroma_client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "    collection = chroma_client.get_collection(\"pokemon_cards\")\n",
    "\n",
    "    # Create a directory to save the vectorized images\n",
    "    output_dir = \"vectorized_images\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load the JSON data\n",
    "    with open(json_file_path, \"r\") as json_file:\n",
    "        cards = json.load(json_file)\n",
    "\n",
    "    # Create a dictionary to map card IDs to their metadata\n",
    "    card_metadata = card_metadata = {\n",
    "        card[\"id\"]: flatten_metadata({k: v for k, v in card.items() if k != \"images\"})\n",
    "        for card in cards\n",
    "    }\n",
    "\n",
    "    # Prepare a list to hold the data to be added to the collection\n",
    "    data_to_add = []\n",
    "\n",
    "    # Vectorize each image in the directory\n",
    "    ids = []\n",
    "    metadatas = []\n",
    "    images = []\n",
    "\n",
    "    for image_file in os.listdir(image_dir):\n",
    "        if image_file.endswith(\".png\"):\n",
    "            card_id = os.path.splitext(image_file)[0]\n",
    "            image_path = os.path.join(image_dir, image_file)\n",
    "\n",
    "            # Prepare the data entry\n",
    "            ids.append(card_id)\n",
    "            images.append(image_path)\n",
    "            if card_id in card_metadata:\n",
    "                metadatas.append(card_metadata[card_id])\n",
    "            else:\n",
    "                metadatas.append({})\n",
    "\n",
    "    # Add all data to the collection at once\n",
    "    collection.add(ids=ids, metadatas=metadatas, images=images)\n",
    "\n",
    "    print(\"Vectorization completed.\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "vectorize_cards(\"images\", \"pokemon-tcg-data-master/cards/en/sm115.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "collection = chroma_client.get_collection(\"pokemon_cards\")\n",
    "\n",
    "results = collection.query(\n",
    "    query_images=[\n",
    "        \"/Users/pabloelgueta/Documents/trading_card_detection/cropped_images/frame_225_crop_4.jpg\"\n",
    "    ], n_results=3\n",
    ")\n",
    "\n",
    "print(\"Query Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_image(url: str):\n",
    "    import requests\n",
    "\n",
    "    # Define the endpoint and parameters\n",
    "    endpoint = \"https://pokemon-cards.cognitiveservices.azure.com/computervision/retrieval:vectorizeImage\"\n",
    "    api_version = \"2024-02-01\"\n",
    "    model_version = \"2023-04-15\"\n",
    "    subscription_key = \"b365927a9ad0473fa1d4054ecd6a77c8\"\n",
    "\n",
    "    # Define the headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Ocp-Apim-Subscription-Key\": subscription_key,\n",
    "    }\n",
    "\n",
    "    # Define the data payload\n",
    "    data = {\n",
    "        \"url\": url\n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(\n",
    "        f\"{endpoint}?api-version={api_version}&model-version={model_version}\",\n",
    "        headers=headers,\n",
    "        json=data,\n",
    "    )\n",
    "\n",
    "    # Print the response\n",
    "    print(response.status_code)\n",
    "    print(response.json())\n",
    "    response = response.json()\n",
    "    return response[\"vector\"]\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input) -> Embeddings:\n",
    "        return [vectorize_image(image) for image in input]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir(\"/Users/pabloelgueta/Documents/trading_card_detection/images\")\n",
    "files = [\n",
    "    os.path.join(\"/Users/pabloelgueta/Documents/trading_card_detection/images\", f)\n",
    "    for f in files\n",
    "    if f.lower().endswith(\".png\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model\n",
    "dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")\n",
    "\n",
    "# Select the device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "dinov2_vits14.to(device)\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_image = T.Compose(\n",
    "    [T.ToTensor(), T.Resize(244), T.CenterCrop(224), T.Normalize([0.5], [0.5])]\n",
    ")\n",
    "\n",
    "\n",
    "def load_image(img: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load an image and return a tensor that can be used as an input to DINOv2.\n",
    "    \"\"\"\n",
    "    img = Image.open(img)\n",
    "\n",
    "    transformed_img = transform_image(img)[:3].unsqueeze(0)\n",
    "\n",
    "    return transformed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(files: list) -> faiss.IndexFlatL2:\n",
    "    \"\"\"\n",
    "    Create an index that contains all of the images in the specified list of files.\n",
    "    \"\"\"\n",
    "    index = faiss.IndexFlatL2(384)\n",
    "\n",
    "    all_embeddings = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, file in enumerate(files):\n",
    "            print(f\"Processing file {i+1}/{len(files)}: {file}\")\n",
    "            embeddings = dinov2_vits14(load_image(file).to(device))\n",
    "\n",
    "            embedding = embeddings[0].cpu().numpy()\n",
    "\n",
    "            all_embeddings[file] = np.array(embedding).reshape(1, -1).tolist()\n",
    "\n",
    "            index.add(np.array(embedding).reshape(1, -1))\n",
    "\n",
    "    with open(\"all_embeddings.json\", \"w\") as f:\n",
    "        f.write(json.dumps(all_embeddings))\n",
    "\n",
    "    faiss.write_index(index, \"data.bin\")\n",
    "\n",
    "    return index, all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index, all_embeddings = create_index(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph_main import _print_event, part_1_graph\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Pokemon Trainer\"\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "_printed = set()\n",
    "\n",
    "events = part_1_graph.stream(\n",
    "    {\n",
    "        \"messages\": (\n",
    "            \"user\",\n",
    "            \"Just use the card identify tool\",\n",
    "        )\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from identify_cards import trigger_crop\n",
    "trigger_crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"audio-classification\", model=\"MIT/ast-finetuned-speech-commands-v2\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines.audio_utils import ffmpeg_microphone_live\n",
    "\n",
    "\n",
    "def launch_fn(\n",
    "    wake_word=\"marvin\",\n",
    "    prob_threshold=0.5,\n",
    "    chunk_length_s=2.0,\n",
    "    stream_chunk_s=0.25,\n",
    "    debug=True,\n",
    "):\n",
    "    if wake_word not in classifier.model.config.label2id.keys():\n",
    "        raise ValueError(\n",
    "            f\"Wake word {wake_word} not in set of valid class labels, pick a wake word in the set {classifier.model.config.label2id.keys()}.\"\n",
    "        )\n",
    "\n",
    "    sampling_rate = classifier.feature_extractor.sampling_rate\n",
    "\n",
    "    mic = ffmpeg_microphone_live(\n",
    "        sampling_rate=sampling_rate,\n",
    "        chunk_length_s=chunk_length_s,\n",
    "        stream_chunk_s=stream_chunk_s,\n",
    "    )\n",
    "\n",
    "    print(\"Listening for wake word...\")\n",
    "    for prediction in classifier(mic):\n",
    "        prediction = prediction[0]\n",
    "        if debug:\n",
    "            print(prediction)\n",
    "        if prediction[\"label\"] == wake_word:\n",
    "            if prediction[\"score\"] > prob_threshold:\n",
    "                return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "from audio_commands import synthesise\n",
    "\n",
    "audio = synthesise(\n",
    "    \"Hugging Face is a company that provides natural language processing and machine learning tools for developers.\"\n",
    ")\n",
    "\n",
    "Audio(audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
